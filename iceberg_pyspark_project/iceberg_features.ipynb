{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/24 07:33:03 WARN Utils: Your hostname, codespaces-785606 resolves to a loopback address: 127.0.0.1; using 10.0.2.119 instead (on interface eth0)\n",
      "25/04/24 07:33:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/workspaces/learn_iceberg/iceberg_pyspark_project/.venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/codespace/.ivy2/cache\n",
      "The jars for the packages stored in: /home/codespace/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0205e18a-7bcf-473a-911a-c1e9db315cd8;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2 in central\n",
      ":: resolution report :: resolve 217ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0205e18a-7bcf-473a-911a-c1e9db315cd8\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/7ms)\n",
      "25/04/24 07:33:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session with Iceberg configurations\n",
    "spark = SparkSession.builder \\\n",
    "  .appName(\"IcebergLocalDevelopment\") \\\n",
    "  .config('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2') \\\n",
    "  .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "  .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "  .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "  .config(\"spark.sql.catalog.local.warehouse\", \"spark-warehouse/iceberg\") \\\n",
    "  .getOrCreate()\n",
    "\n",
    "# Verify Spark session creation\n",
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/21 10:37:40 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read data from the sample file\n",
    "df = spark.read.csv(\"/workspaces/learn_iceberg/iceberg_pyspark_project/customers-5L.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Index',\n",
       " 'Customer Id',\n",
       " 'First Name',\n",
       " 'Last Name',\n",
       " 'Company',\n",
       " 'City',\n",
       " 'Country',\n",
       " 'Phone 1',\n",
       " 'Phone 2',\n",
       " 'Email',\n",
       " 'Subscription Date',\n",
       " 'Website']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename all the columns\n",
    "df = df.withColumnRenamed(\"Customer Id\", \"Customer_Id\")\n",
    "df = df.withColumnRenamed(\"First Name\", \"First_Name\")\n",
    "df = df.withColumnRenamed(\"Last Name\", \"Last_Name\")\n",
    "df = df.withColumnRenamed(\"Phone 1\", \"Phone_1\")\n",
    "df = df.withColumnRenamed(\"Phone 2\", \"Phone_2\")\n",
    "df = df.withColumnRenamed(\"Subscription Date\", \"Subscription_Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/17 10:30:52 WARN HadoopTableOperations: Error reading version hint file spark-warehouse/iceberg/schema/customers_5k/metadata/version-hint.text\n",
      "java.io.FileNotFoundException: File spark-warehouse/iceberg/schema/customers_5k/metadata/version-hint.text does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.findVersion(HadoopTableOperations.java:318)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.refresh(HadoopTableOperations.java:104)\n",
      "\tat org.apache.iceberg.BaseTransaction.lambda$commitReplaceTransaction$1(BaseTransaction.java:368)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitReplaceTransaction(BaseTransaction.java:365)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitTransaction(BaseTransaction.java:314)\n",
      "\tat org.apache.iceberg.CommitCallbackTransaction.commitTransaction(CommitCallbackTransaction.java:126)\n",
      "\tat org.apache.iceberg.spark.source.StagedSparkTable.commitStagedChanges(StagedSparkTable.java:34)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:585)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:578)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:572)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:186)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:221)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.internalReplace(DataFrameWriterV2.scala:213)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.createOrReplace(DataFrameWriterV2.scala:135)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/04/17 10:30:52 WARN HadoopTableOperations: Error reading version hint file spark-warehouse/iceberg/schema/customers_5k/metadata/version-hint.text\n",
      "java.io.FileNotFoundException: File spark-warehouse/iceberg/schema/customers_5k/metadata/version-hint.text does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n",
      "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.findVersion(HadoopTableOperations.java:318)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.refresh(HadoopTableOperations.java:104)\n",
      "\tat org.apache.iceberg.hadoop.HadoopTableOperations.current(HadoopTableOperations.java:84)\n",
      "\tat org.apache.iceberg.BaseTransaction.lambda$commitReplaceTransaction$1(BaseTransaction.java:377)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n",
      "\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitReplaceTransaction(BaseTransaction.java:365)\n",
      "\tat org.apache.iceberg.BaseTransaction.commitTransaction(BaseTransaction.java:314)\n",
      "\tat org.apache.iceberg.CommitCallbackTransaction.commitTransaction(CommitCallbackTransaction.java:126)\n",
      "\tat org.apache.iceberg.spark.source.StagedSparkTable.commitStagedChanges(StagedSparkTable.java:34)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:585)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:578)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:572)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:186)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:221)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:201)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.internalReplace(DataFrameWriterV2.scala:213)\n",
      "\tat org.apache.spark.sql.DataFrameWriterV2.createOrReplace(DataFrameWriterV2.scala:135)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    }
   ],
   "source": [
    "# Create iceberg table with limmited number of records.\n",
    "df.limit(5000).writeTo(\"local.schema.customers_5k\") \\\n",
    "  .using(\"iceberg\") \\\n",
    "  .createOrReplace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+----------+----------+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+-----------------+--------------------+\n",
      "|Index|    Customer_Id|First_Name| Last_Name|             Company|              City|             Country|             Phone_1|             Phone_2|               Email|Subscription_Date|             Website|\n",
      "+-----+---------------+----------+----------+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+-----------------+--------------------+\n",
      "|   51|803F5AaF0DdeaB1|  Lorraine|     Wyatt|      Levine-Jimenez|        Gateshaven|United Arab Emirates|001-712-043-1865x...|     +1-527-236-9562|uwarren@cameron.info|       2020-02-02|https://www.pache...|\n",
      "|  103|44FdbD9aDDbea9B|     Diane|     Wolfe|    Peterson-Schmitt|    Port Gordonton|               Malta| +1-858-307-2119x750|+1-377-124-3750x4...|andresdodson@khan...|       2020-02-05|    http://luna.com/|\n",
      "|  108|D2d0Dbd16aBecca|      Seth|    Duncan|         Huffman PLC|        West Paige|            Dominica|   (488)847-1071x862|001-771-009-8998x485|  kperkins@grant.com|       2022-02-26|http://www.bryan....|\n",
      "|  109|7Bb38741aADbDcf|   Zachary|    Savage|          Stuart Inc|    Penningtonview|           Nicaragua| (599)425-8966x66778|     +1-451-335-4053|wayne87@sutton-ha...|       2020-02-24|https://www.kim.com/|\n",
      "|  118|F7ABEa08EDFde90|     Becky|     Owens|         Parker-Ford|    Lake Dianaland|British Virgin Is...|   (397)653-1062x583|001-420-366-2394x...|   wbaldwin@yang.com|       2021-02-15|https://melton-pr...|\n",
      "|  120|4Be5262C7721Cbb|      Ivan|     Costa|Stein, Davis and ...|Lake Franciscofort|             Hungary|    001-812-618-9942|    402-775-3066x127|mfoley@velez-gain...|       2021-02-14| http://duarte.info/|\n",
      "|  122|866479A4f6d5Ef0|     Eddie|  Atkinson|      Sherman-Savage|        Ravenhaven|Heard Island and ...|       (254)576-7122|   190-781-2339x6577|victorbernard@ben...|       2020-02-26|https://www.baird...|\n",
      "|  123|BC1Ba846Ac3E48B| Alejandro|     Moran|Perry, Huber and ...|    South Theodore|    Saint Barthelemy|+1-268-838-4287x4...| +1-313-303-9898x138|  dunnbobby@snow.com|       2021-02-11|http://www.crosby...|\n",
      "|  137|aF6f1Bd253AAe9A|    Sydney|    Santos|Ewing, Clark and ...|        Tammyville|          Montenegro|   618-585-4911x7339|    408-283-7073x377|alejandroclark@mi...|       2020-02-23|https://montoya-j...|\n",
      "|  143|e6bbcb2038A3910|  Fernando|    Watson|       May-Mccormick|           Kurtton|             Estonia|001-977-384-2896x...|  (436)304-2074x5312|jessicahebert@ewi...|       2020-02-21|https://www.niels...|\n",
      "|  149|FE3DA460f70e61F|     Lydia|   Barajas|Pope, Chavez and ...|       New Cameron|       Liechtenstein|   210-148-0978x0620|+1-422-547-3315x6...|neilvelasquez@lop...|       2022-02-04|https://www.holme...|\n",
      "|  150|F27b20EeAfcE104|   Belinda|Cunningham|       Mcconnell PLC|      Kelliborough|United States Min...|          0907912007|   (341)116-2976x424|joelfaulkner@hays...|       2022-02-07|https://www.mccal...|\n",
      "|  158|7837b6Fc7E4c3A0|    Philip| Jefferson|      Kaufman-Warren|           New Ann|         Saint Lucia|       (684)980-9455|  507-010-9949x87668|latoya40@reid-leo...|       2021-02-15|http://www.sweene...|\n",
      "|  160|409F33D08E8B9AE|     Chloe|    Ibarra|Preston, Chaney a...|       North Chloe|            Tanzania|    463.878.0229x307|    765.274.4554x393|zstewart@summers-...|       2022-02-10|  http://norris.com/|\n",
      "|  176|FBd3Cd772f9a5Ad|  Franklin|     Greer|Mckinney, Acevedo...|        Castroland|               Italy|001-584-962-2188x...|+1-334-854-1255x1...|  rjenkins@huang.com|       2021-02-13| https://mason.info/|\n",
      "|  189|fEbCEDEB7B8264e|    Karina|      Page|         Hawkins PLC|        Hunterbury|            Mongolia|001-164-500-2904x...|   949-175-2052x9372|nataliespence@and...|       2022-02-10|http://www.morgan...|\n",
      "|  198|ebccD3CF61FdCde|    Jeanne| Gutierrez|Huff, Vance and Wong|   East Ashleeberg|               Nauru|001-408-945-5693x...|        208.577.6465|pnguyen@donaldson...|       2022-02-26|https://rollins.net/|\n",
      "|  215|cADa29B91aC0E9B|   Kristen|   Miranda|       Holmes-Strong|         Ryanshire|           Singapore|        039.419.5255|          2570304810| lmays@armstrong.org|       2021-02-20|https://www.clark...|\n",
      "|  217|52AEeB25Ed3E4F7|     Chase|  Garrison|          Burke-Hall|           Beanton|               Yemen|       (073)922-2233|        905.829.5933|  jbaxter@walker.net|       2021-02-05|http://mayer-wong...|\n",
      "|  235|52CC22fDbdb691C|      Evan|     Moody|        Bowen-Joseph|        New Jeanne|             Eritrea|001-919-365-4511x...|        856.907.7792|bnavarro@macdonal...|       2022-02-04|https://norton-st...|\n",
      "+-----+---------------+----------+----------+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select data from the created table without partition\n",
    "spark.sql(\"select * from local.schema.customers_5k where month(Subscription_Date) = '2'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Optimized Logical Plan ==\n",
      "CommandResult [plan#257], Execute ExplainCommand, [[== Physical Plan ==\n",
      "*(1) Filter (isnotnull(Subscription_Date#272) AND (month(Subscription_Date#272) = 2))\n",
      "+- *(1) ColumnarToRow\n",
      "   +- BatchScan local.schema.customers_5k[Index#262, Customer_Id#263, First_Name#264, Last_Name#265, Company#266, City#267, Country#268, Phone_1#269, Phone_2#270, Email#271, Subscription_Date#272, Website#273] local.schema.customers_5k (branch=null) [filters=Subscription_Date IS NOT NULL, groupedBy=] RuntimeFilters: []\n",
      "\n",
      "]], Statistics(sizeInBytes=28.0 B)\n",
      "   +- ExplainCommand 'Project [*], SimpleMode\n",
      "\n",
      "== Physical Plan ==\n",
      "CommandResult [plan#257]\n",
      "   +- Execute ExplainCommand\n",
      "         +- ExplainCommand 'Project [*], SimpleMode\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explain the plan for the above query\n",
    "spark.sql(\"explain select * from local.schema.customers_5k where month(Subscription_Date) = '2'\").explain(mode=\"cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create iceberg table with partition\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE local.schema.customers_partitioned_by_date(\n",
    "  Index INT,\n",
    "  Customer_Id STRING,\n",
    "  First_Name STRING,\n",
    "  Last_Name STRING,\n",
    "  Company STRING,\n",
    "  City STRING,\n",
    "  Country STRING,\n",
    "  Phone_1 STRING,\n",
    "  Phone_2 STRING,\n",
    "  Email STRING,\n",
    "  Subscription_Date DATE,\n",
    "  Website STRING\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (months(Subscription_Date))\n",
    "LOCATION 'spark-warehouse/iceberg/schema/customers_partitioned_by_date'\n",
    "TBLPROPERTIES (\n",
    "  'format' = 'iceberg/parquet',\n",
    "  'format-version' = '2',\n",
    "  'write.parquet.compression-codec' = 'zstd'\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# write data to partitioned table\n",
    "df.limit(5000).writeTo(\"local.schema.customers_partitioned_by_date\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+----------+---------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------------+--------------------+\n",
      "|Index|    Customer_Id|First_Name|Last_Name|             Company|               City|             Country|             Phone_1|             Phone_2|               Email|Subscription_Date|             Website|\n",
      "+-----+---------------+----------+---------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------------+--------------------+\n",
      "|    2|6EDdBA3a2DFA7De|    Yvonne|     Shaw|     Jensen and Sons|          Janetfort|Palestinian Terri...|          9610730173|   531-482-3000x7085|  kleinluis@vang.com|       2021-01-01|https://www.paul....|\n",
      "| 1405|dD7012CFFa1dbED|    Steven|   Dunlap|        Horne-Conway|  New Autumnborough|            Guernsey|    133.063.0367x635|        711.847.2122|  ehorne@henson.info|       2021-01-01|https://www.gomez...|\n",
      "| 1536|2b1309DCa442D39|    Yvonne|  Bradley|        Avery-Grimes|           Leonberg|              Poland|          6844072977|    356-244-9125x132|colemansylvia@ort...|       2021-01-01|http://www.carey-...|\n",
      "| 1540|8B731A9dc1E3481|     Traci|    Reyes|            Todd LLC|       South Ashlee|       Faroe Islands|001-648-120-5169x...| (873)186-0401x51802|qmarshall@hollowa...|       2021-01-01| https://baxter.com/|\n",
      "| 1577|46699C9bfEa07d2|      Brad|  Carroll|Cooke, Keith and ...|         Conwayland|            Bulgaria| +1-877-318-2507x813|001-343-488-9472x...|    awolf@clarke.com|       2021-01-01|http://www.seller...|\n",
      "| 1852|0AE003B7b352Eb9|      Kent|    Casey|        Cisneros Inc|          Parkshire|                Guam|+1-798-000-2210x2682|    501-720-2682x208|ruthspence@snyder...|       2021-01-01|   https://hood.com/|\n",
      "| 2061|5B3BfB30d123Efc|     Edwin|     Soto|            Cruz Inc|        Pollardfort|            Cambodia|        692.375.9622|  841.630.8300x31423|glassevelyn@munoz...|       2021-01-01|https://fuentes-w...|\n",
      "| 2222|46B651F51bDF1BC|     Caleb| Thornton|Brooks, Bryan and...|West Cassandramouth|             Iceland|   369-909-0455x3195|     +1-830-680-6963|donaldchaney@cort...|       2021-01-01|   http://irwin.com/|\n",
      "| 2438|1d0c1Bf99420f18| Gabrielle|  Whitney|            Soto LLC|     New Yvetteport|              Malawi|       (592)441-1823|          8233128185|   eddie43@avila.com|       2021-01-01|http://www.banks....|\n",
      "| 2905|282DE9beb1faa44|   Kristen|   Wright|Ramsey, Huynh and...|         Deckerfurt|               Gabon|       (075)492-6680|+1-626-458-2732x0...|barbara57@sherman...|       2021-01-01| http://www.cox.com/|\n",
      "| 3638|Cf818120bC3082f|     Jesus|   Waters|         Salas Group|        Spenceshire|                Iraq|001-483-553-6149x...|          2711654686|ralphgrimes@cueva...|       2021-01-01|http://www.nguyen...|\n",
      "| 4252|ceaf75897981Bf6|      Kara|     Sims|      Lloyd and Sons|        Conleyshire|           Venezuela|   190-470-6651x9941|     +1-950-339-4270|     pkirby@bean.biz|       2021-01-01|https://www.holt....|\n",
      "| 4330|dE047D17A84d4cA| Alejandro|    Pitts|           Hurst LLC|       Copelandberg|Saint Vincent and...|       (254)814-2365|  689-846-8915x13519|xhuerta@thompson-...|       2021-01-01|   http://owens.org/|\n",
      "| 4631|Be5C3D83C4E46A0|   Raymond|Nicholson|Gutierrez, Mcmaho...|         Port Ricky|  Dominican Republic|        438-834-2832|+1-978-582-7763x4...|larryhenderson@es...|       2021-01-01|https://galloway....|\n",
      "+-----+---------------+----------+---------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute\n",
    "spark.sql(\"SELECT * FROM local.schema.customers_partitioned_by_date WHERE Subscription_Date = '2021'\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Optimized Logical Plan ==\n",
      "CommandResult [plan#382], Execute ExplainCommand, [[== Physical Plan ==\n",
      "*(1) Filter (Subscription_Date#397 = 2021-01-01)\n",
      "+- *(1) ColumnarToRow\n",
      "   +- BatchScan local.schema.customers_partitioned_by_date[Index#387, Customer_Id#388, First_Name#389, Last_Name#390, Company#391, City#392, Country#393, Phone_1#394, Phone_2#395, Email#396, Subscription_Date#397, Website#398] local.schema.customers_partitioned_by_date (branch=null) [filters=Subscription_Date IS NOT NULL, Subscription_Date = 18628, groupedBy=] RuntimeFilters: []\n",
      "\n",
      "]], Statistics(sizeInBytes=28.0 B)\n",
      "   +- ExplainCommand 'Project [*], SimpleMode\n",
      "\n",
      "== Physical Plan ==\n",
      "CommandResult [plan#382]\n",
      "   +- Execute ExplainCommand\n",
      "         +- ExplainCommand 'Project [*], SimpleMode\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explain the plan for the above query\n",
    "spark.sql(\"explain select * from local.schema.customers_partitioned_by_date where Subscription_Date = '2021'\").explain(mode=\"cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Optimized Logical Plan ==\n",
      "CommandResult [plan#1269], Execute ExplainCommand, [[== Physical Plan ==\n",
      "*(1) Filter (month(Subscription_Date#1284) = 2)\n",
      "+- *(1) ColumnarToRow\n",
      "   +- BatchScan local.schema.customers_partitioned_by_date[Index#1274, Customer_Id#1275, First_Name#1276, Last_Name#1277, Company#1278, City#1279, Country#1280, Phone_1#1281, Phone_2#1282, Email#1283, Subscription_Date#1284, Website#1285] local.schema.customers_partitioned_by_date (branch=null) [filters=Subscription_Date IS NOT NULL, groupedBy=] RuntimeFilters: []\n",
      "\n",
      "]], Statistics(sizeInBytes=28.0 B)\n",
      "   +- ExplainCommand 'Project [*], SimpleMode\n",
      "\n",
      "== Physical Plan ==\n",
      "CommandResult [plan#1269]\n",
      "   +- Execute ExplainCommand\n",
      "         +- ExplainCommand 'Project [*], SimpleMode\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explain the plan for the above query\n",
    "spark.sql(\"explain select * from local.schema.customers_partitioned_by_date where month(Subscription_Date) = '02'\").explain(mode=\"cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Optimized Logical Plan ==\n",
      "CommandResult [plan#1303], Execute ExplainCommand, [[== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- BatchScan local.schema.customers_partitioned_by_date[Index#1308, Customer_Id#1309, First_Name#1310, Last_Name#1311, Company#1312, City#1313, Country#1314, Phone_1#1315, Phone_2#1316, Email#1317, Subscription_Date#1318, Website#1319] local.schema.customers_partitioned_by_date (branch=null) [filters=Subscription_Date IS NOT NULL, Subscription_Date >= 18659, Subscription_Date < 18687, groupedBy=] RuntimeFilters: []\n",
      "\n",
      "]], Statistics(sizeInBytes=28.0 B)\n",
      "   +- ExplainCommand 'Project [*], SimpleMode\n",
      "\n",
      "== Physical Plan ==\n",
      "CommandResult [plan#1303]\n",
      "   +- Execute ExplainCommand\n",
      "         +- ExplainCommand 'Project [*], SimpleMode\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"explain select * from local.schema.customers_partitioned_by_date WHERE Subscription_Date >= '2021-02-01' AND Subscription_Date < '2021-03-01'\").explain(mode=\"cost\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------------------------+-------+\n",
      "|col_name          |data_type                          |comment|\n",
      "+------------------+-----------------------------------+-------+\n",
      "|Index             |int                                |NULL   |\n",
      "|Customer_Id       |string                             |NULL   |\n",
      "|First_Name        |string                             |NULL   |\n",
      "|Last_Name         |string                             |NULL   |\n",
      "|Company           |string                             |NULL   |\n",
      "|City              |string                             |NULL   |\n",
      "|Country           |string                             |NULL   |\n",
      "|Phone_1           |string                             |NULL   |\n",
      "|Phone_2           |string                             |NULL   |\n",
      "|Email             |string                             |NULL   |\n",
      "|Subscription_Date |date                               |NULL   |\n",
      "|Website           |string                             |NULL   |\n",
      "|                  |                                   |       |\n",
      "|# Partitioning    |                                   |       |\n",
      "|Part 0            |months(Subscription_Date)          |       |\n",
      "|                  |                                   |       |\n",
      "|# Metadata Columns|                                   |       |\n",
      "|_spec_id          |int                                |       |\n",
      "|_partition        |struct<Subscription_Date_month:int>|       |\n",
      "|_file             |string                             |       |\n",
      "+------------------+-----------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE FORMATTED local.schema.customers_partitioned_by_date\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create iceberg table with TRUNCATE transformation\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE local.schema.customers_partitioned_by_name (\n",
    "  Index INT,\n",
    "  Customer_Id STRING,\n",
    "  First_Name STRING,\n",
    "  Last_Name STRING,\n",
    "  Company STRING,\n",
    "  City STRING,\n",
    "  Country STRING,\n",
    "  Phone_1 STRING,\n",
    "  Phone_2 STRING,\n",
    "  Email STRING,\n",
    "  Subscription_Date DATE,\n",
    "  Website STRING\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (truncate(Last_Name,5))\n",
    "LOCATION 'spark-warehouse/iceberg/schema/customers_partitioned_by_name'\n",
    "TBLPROPERTIES (\n",
    "  'format' = 'iceberg/parquet',\n",
    "  'format-version' = '2',\n",
    "  'write.parquet.compression-codec' = 'zstd'\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.limit(200).writeTo(\"local.schema.customers_partitioned_by_name\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+----------+---------+------------+----------+--------------------+--------------------+---------------+--------------------+-----------------+------------------+\n",
      "|Index|    Customer_Id|First_Name|Last_Name|     Company|      City|             Country|             Phone_1|        Phone_2|               Email|Subscription_Date|           Website|\n",
      "+-----+---------------+----------+---------+------------+----------+--------------------+--------------------+---------------+--------------------+-----------------+------------------+\n",
      "|  101|dda3Da93Db3fA71|    Gordon|   Abbott|Wall-Bradley|Vernonfurt|United Arab Emirates|+1-587-566-4321x2456|+1-849-626-5599|cristina14@miller...|       2020-06-23|http://butler.com/|\n",
      "+-----+---------------+----------+---------+------------+----------+--------------------+--------------------+---------------+--------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from local.schema.customers_partitioned_by_name where Last_Name = 'Abbott'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Optimized Logical Plan ==\n",
      "CommandResult [plan#257], Execute ExplainCommand, [[== Physical Plan ==\n",
      "*(1) Filter (Last_Name#265 = Abbott)\n",
      "+- *(1) ColumnarToRow\n",
      "   +- BatchScan local.schema.customers_partitioned_by_name[Index#262, Customer_Id#263, First_Name#264, Last_Name#265, Company#266, City#267, Country#268, Phone_1#269, Phone_2#270, Email#271, Subscription_Date#272, Website#273] local.schema.customers_partitioned_by_name (branch=null) [filters=Last_Name IS NOT NULL, Last_Name = 'Abbott', groupedBy=] RuntimeFilters: []\n",
      "\n",
      "]], Statistics(sizeInBytes=28.0 B)\n",
      "   +- ExplainCommand 'Project [*], SimpleMode\n",
      "\n",
      "== Physical Plan ==\n",
      "CommandResult [plan#257]\n",
      "   +- Execute ExplainCommand\n",
      "         +- ExplainCommand 'Project [*], SimpleMode\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"explain select * from local.schema.customers_partitioned_by_name where Last_Name = 'Abbott'\").explain(mode = \"cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE local.schema.customers_partitioned_by_bucket (\n",
    "  Index INT,\n",
    "  Customer_Id STRING,\n",
    "  First_Name STRING,\n",
    "  Last_Name STRING,\n",
    "  Company STRING,\n",
    "  City STRING,\n",
    "  Country STRING,\n",
    "  Phone_1 STRING,\n",
    "  Phone_2 STRING,\n",
    "  Email STRING,\n",
    "  Subscription_Date DATE,\n",
    "  Website STRING\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (bucket(15,Customer_Id))\n",
    "LOCATION 'spark-warehouse/iceberg/schema/customers_partitioned_by_bucket'\n",
    "TBLPROPERTIES (\n",
    "  'format' = 'iceberg/parquet',\n",
    "  'format-version' = '2',\n",
    "  'write.parquet.compression-codec' = 'zstd'\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.limit(500).writeTo(\"local.schema.customers_partitioned_by_bucket\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+----------+---------+--------------------+-----------------+--------------+--------------------+--------------------+--------------------+-----------------+--------------------+\n",
      "|Index|    Customer_Id|First_Name|Last_Name|             Company|             City|       Country|             Phone_1|             Phone_2|               Email|Subscription_Date|             Website|\n",
      "+-----+---------------+----------+---------+--------------------+-----------------+--------------+--------------------+--------------------+--------------------+-----------------+--------------------+\n",
      "|   14|bB7E28Aa85cbDD3|  Jonathan|    Morse|         Potter-Holt|       Mcgeeburgh|Western Sahara|    054.140.9190x178|   461-138-3043x6409|tami64@rice-mclau...|       2021-06-20|https://www.patri...|\n",
      "|   28|2F1E68Ed8ecDA2a|  Stefanie|   Gibson|       Wheeler Group|       Dillonside|       Armenia|       (246)767-7316|          8183240781|   edward62@rios.net|       2021-04-03|https://www.wells...|\n",
      "|   60|0ca9e45c5D26A0C|    Tracey|      Day|Beltran, Sutton a...|   South Kathleen|         Japan|  368.561.5670x76325|  581.378.5910x81342|   judith34@pham.com|       2021-11-08|https://owens-sta...|\n",
      "|   98|c755fc9aCE45484|    Hayden|  Hartman|       Dominguez Ltd|    Lake Virginia| Liechtenstein| +1-631-213-3611x615| (370)733-7838x28396|waltersadrian@slo...|       2020-01-29|https://www.salin...|\n",
      "|  135|BFEFe1374C1E91B|     Tracy|Robertson|Hardy, Holder and...|       Evelynfurt|         Korea|  (657)961-3847x0417|   (050)615-0925x657|patricknina@hayes...|       2020-03-16|  http://watson.com/|\n",
      "|  147|3Ffb5cFdAc5A44B|   Melinda|  Woodard|       Black-Farrell|    Port Lukefurt|    Costa Rica|001-242-019-0387x666|   198-390-9141x4384|dawsonernest@rams...|       2021-08-20|    http://lara.com/|\n",
      "|  148|D47fE14b2a67DA3|   Lindsay|  Marquez|   Galvan-Richardson| North Pedrohaven|   Switzerland|   (199)844-4148x238|   021-427-9049x4535|bethbonilla@chamb...|       2021-12-18|https://www.barne...|\n",
      "|  152|78fA1B9E88FA6ae|     Robyn|Macdonald|         Howard-Beck|        Rivasland|   Puerto Rico|  (739)642-8712x0508|001-202-916-9538x350|benjamintara@zuni...|       2021-04-13|http://www.bates....|\n",
      "|  173|3Bf0DeCDB6da4Fc|    Tanner|    Moran|          Valdez Ltd|         Troyside|       Nigeria|  (953)344-2428x8489|        644.519.4302|  opatton@durham.com|       2020-10-29|http://www.kenned...|\n",
      "|  184|6ee4aAdcC2f0Ede|     Lydia|  Jenkins|      Rangel-Schmidt|New Daniellemouth|       Vietnam|   973.799.3120x3096|     +1-633-188-8244|montgomerybilly@h...|       2022-05-23|  https://kline.biz/|\n",
      "+-----+---------------+----------+---------+--------------------+-----------------+--------------+--------------------+--------------------+--------------------+-----------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from local.schema.customers_partitioned_by_bucket \").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Optimized Logical Plan ==\n",
      "CommandResult [plan#258], Execute ExplainCommand, [[== Physical Plan ==\n",
      "*(1) Filter (Customer_Id#264 = b9Da13bedEc47de)\n",
      "+- *(1) ColumnarToRow\n",
      "   +- BatchScan local.schema.customers_partitioned_by_bucket[Index#263, Customer_Id#264, First_Name#265, Last_Name#266, Company#267, City#268, Country#269, Phone_1#270, Phone_2#271, Email#272, Subscription_Date#273, Website#274] local.schema.customers_partitioned_by_bucket (branch=null) [filters=Customer_Id IS NOT NULL, Customer_Id = 'b9Da13bedEc47de', groupedBy=] RuntimeFilters: []\n",
      "\n",
      "]], Statistics(sizeInBytes=28.0 B)\n",
      "   +- ExplainCommand 'Project [*], SimpleMode\n",
      "\n",
      "== Physical Plan ==\n",
      "CommandResult [plan#258]\n",
      "   +- Execute ExplainCommand\n",
      "         +- ExplainCommand 'Project [*], SimpleMode\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"explain select * from local.schema.customers_partitioned_by_bucket where Customer_Id = 'b9Da13bedEc47de'\").explain(mode = \"cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE local.schema.customers_partitioned_by_country_city (\n",
    "  Index INT,\n",
    "  Customer_Id STRING,\n",
    "  First_Name STRING,\n",
    "  Last_Name STRING,\n",
    "  Company STRING,\n",
    "  City STRING,\n",
    "  Country STRING,\n",
    "  Phone_1 STRING,\n",
    "  Phone_2 STRING,\n",
    "  Email STRING,\n",
    "  Subscription_Date DATE,\n",
    "  Website STRING\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (Country)\n",
    "LOCATION 'spark-warehouse/iceberg/schema/customers_partitioned_by_country_city'\n",
    "TBLPROPERTIES (\n",
    "  'format' = 'iceberg/parquet',\n",
    "  'format-version' = '2',\n",
    "  'write.parquet.compression-codec' = 'zstd'\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.limit(5000).writeTo(\"local.schema.customers_partitioned_by_country_city\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"ALTER TABLE local.schema.customers_partitioned_by_country_city ADD PARTITION FIELD City\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.limit(100).writeTo(\"local.schema.customers_partitioned_by_country_city\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"ALTER TABLE local.schema.customers_partitioned_by_country_city ADD PARTITION FIELD year(Subscription_Date)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.limit(100).writeTo(\"local.schema.customers_partitioned_by_country_city\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+-------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|committed_at           |snapshot_id        |parent_id          |operation|manifest_list                                                                                                                                     |summary                                                                                                                                                                                                                                                                                                           |\n",
      "+-----------------------+-------------------+-------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2025-04-21 10:29:54.179|396991467446470913 |NULL               |append   |spark-warehouse/iceberg/schema/customers_partitioned_by_country_city/metadata/snap-396991467446470913-1-22c73ee7-98a0-42f8-8035-5be702a4c318.avro |{spark.app.id -> local-1745229907020, added-data-files -> 243, added-records -> 5000, added-files-size -> 1410726, changed-partition-count -> 243, total-records -> 5000, total-files-size -> 1410726, total-data-files -> 243, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|2025-04-21 10:38:14.031|7386602405767638480|396991467446470913 |append   |spark-warehouse/iceberg/schema/customers_partitioned_by_country_city/metadata/snap-7386602405767638480-1-c4c9fd2e-92a6-4aba-9e61-f649047f64ca.avro|{spark.app.id -> local-1745231847398, added-data-files -> 100, added-records -> 100, added-files-size -> 388421, changed-partition-count -> 100, total-records -> 5100, total-files-size -> 1799147, total-data-files -> 343, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}  |\n",
      "|2025-04-21 10:44:40.724|1590301241980826574|7386602405767638480|append   |spark-warehouse/iceberg/schema/customers_partitioned_by_country_city/metadata/snap-1590301241980826574-1-35573ccb-618e-4300-b664-fa453365bcbd.avro|{spark.app.id -> local-1745231847398, added-data-files -> 100, added-records -> 100, added-files-size -> 388421, changed-partition-count -> 100, total-records -> 5200, total-files-size -> 2187568, total-data-files -> 443, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}  |\n",
      "+-----------------------+-------------------+-------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM local.schema.customers_partitioned_by_country_city.snapshots\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+----------+----------+--------------------+--------------------+-------+--------------------+--------------------+--------------------+-----------------+--------------------+\n",
      "|Index|    Customer_Id|First_Name| Last_Name|             Company|                City|Country|             Phone_1|             Phone_2|               Email|Subscription_Date|             Website|\n",
      "+-----+---------------+----------+----------+--------------------+--------------------+-------+--------------------+--------------------+--------------------+-----------------+--------------------+\n",
      "|  157|8dBcFDF5ED9D5D2|    Evelyn|   Krueger|     Kelley and Sons|     East Coreyburgh|Lesotho|   (872)960-8087x638|001-729-045-6337x...|emmacameron@sloan...|       2021-04-24|http://www.valenz...|\n",
      "|  236|c0Ea931f3eBb4A5|     Jerry|     Evans|        Velez-Keller|   Port Tracyborough|Lesotho|    785.352.8872x837|+1-848-210-2482x7907|kingkaitlin@mccal...|       2021-12-18|https://www.farre...|\n",
      "|  342|3fbd61a23A8f77A|      Tara|   Harrell|        Cantrell Ltd|      Lake Codymouth|Lesotho|    141-144-3312x223|          9561171411|     ohuber@frye.net|       2021-05-14|http://www.pierce...|\n",
      "|  512|669EBad72e3510F|  Brittany|   Brennan|            Yu Group|       Barberchester|Lesotho|001-319-191-5050x495|       (059)313-1921|underwoodreginald...|       2020-11-18|http://barry-hurs...|\n",
      "|  665|861bbe481B1c14E|    Elijah|  Matthews|   Fletcher and Sons|West Gwendolynbor...|Lesotho|    844-418-5440x531|+1-814-705-5464x8...|  danthony@kline.com|       2020-11-04|http://www.olson....|\n",
      "|  701|B2d25C904230f0C|      Ross|     Booth|Carter, Shepherd ...|           Damonview|Lesotho|        418.961.3832| (775)301-2787x44575|    chodge@casey.com|       2020-03-02|  https://short.org/|\n",
      "| 1228|9e3b2e960Bb92eB|      Seth|    Knight|     Cabrera-Hammond|          Howardberg|Lesotho|   (742)774-5191x270| (728)120-9454x61139|shannontaylor@com...|       2020-11-22|https://watts-boo...|\n",
      "| 1272|eDC0e4A79Fc54bD|     Kayla|  Schaefer|Wang, Livingston ...|          Port Stacy|Lesotho|       (513)657-6247| (071)921-4592x30508|dunnariana@huff-c...|       2020-07-09|   https://ball.biz/|\n",
      "| 1434|9970aF4EeEb07F7|    Regina|     Keith|Keller, Meyers an...|    Port Geneborough|Lesotho|001-567-649-6544x...|          8170418736|    erin95@banks.org|       2020-11-25|https://ramos-all...|\n",
      "| 1759|A9d20092bbfE50f|     Danny|  Martinez|           Tyler LLC|        Caitlinshire|Lesotho|001-360-394-2348x...|+1-176-304-3961x4...|blairfrank@rowlan...|       2021-05-25|http://www.prince...|\n",
      "| 1776|C5fF46CfbC4315e|     Edgar|   Donovan|         Parks-Sloan|         Jodiborough|Lesotho|    734-143-9952x951|        396.049.2274|hunterana@michael...|       2020-10-26|http://www.maynar...|\n",
      "| 2111|B1374e4868a197C|     Allen|Valenzuela|        Gallegos LLC|        South Sandra|Lesotho|          4695348941|+1-883-771-6758x4330|stantonallison@ya...|       2020-07-31|http://www.garner...|\n",
      "| 2129|9dAFfFE3f229524|      Alex|   Everett|Baker, Ellison an...|          Port Diana|Lesotho| (133)204-4607x06078|+1-082-134-9217x0597|clarkmarisa@sims.com|       2021-03-21|https://cervantes...|\n",
      "| 2307|ee8AeB4882035CC|       Amy|   Goodman|Gentry, Lester an...|          Rubenmouth|Lesotho| +1-452-497-2577x723|       (516)921-1978|isabelmccormick@w...|       2021-10-20|https://www.hampt...|\n",
      "| 2378|ACACB8F42c2C791|     Kevin|    Maddox|     Savage and Sons|    South Marvinland|Lesotho|001-674-645-3315x...|    205-902-2191x416|kathrynanderson@r...|       2020-03-19|https://hanson.info/|\n",
      "| 2625|7Cc036BDA9FDecb|   Jasmine|      Frey|Walsh, Barrett an...|        Darrellmouth|Lesotho|          8605024060|   (122)834-4012x991|vmedina@riddle-ki...|       2021-02-27|  https://chung.com/|\n",
      "| 2627|e4c4d7cec29eAff|      Jodi|      Clay|     Hebert and Sons|        Jacksonmouth|Lesotho|       (231)518-6940|  (296)670-6351x8689|keith70@hughes-hu...|       2020-01-09|https://stricklan...|\n",
      "| 4335|BFcE1f201aeBD35|      Kirk| Schneider|       Hickman Group|         West Kelsey|Lesotho|001-913-846-1788x096|        564-552-1338|jorge22@chung-mar...|       2022-04-23|http://www.buckle...|\n",
      "| 4950|EDBFeDBd6375944|    Vernon|    Gamble|          West Group|       Port Kathleen|Lesotho|    001-199-305-9359|   971.844.4679x1176|   qterrell@rowe.com|       2020-12-05|http://www.odonne...|\n",
      "|  210|7Fd8465F2E8F6Fb| Dominique|   Lindsey|          Mcneil PLC|   West Wendychester|Algeria|001-003-500-7259x...|        851-108-8869|  yayers@vasquez.com|       2022-03-28|   http://berry.com/|\n",
      "+-----+---------------+----------+----------+--------------------+--------------------+-------+--------------------+--------------------+--------------------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM local.schema.customers_partitioned_by_country_city VERSION AS OF 396991467446470913;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+----------+----------+--------------------+--------------------+-------+--------------------+--------------------+--------------------+-----------------+--------------------+\n",
      "|Index|    Customer_Id|First_Name| Last_Name|             Company|                City|Country|             Phone_1|             Phone_2|               Email|Subscription_Date|             Website|\n",
      "+-----+---------------+----------+----------+--------------------+--------------------+-------+--------------------+--------------------+--------------------+-----------------+--------------------+\n",
      "|  157|8dBcFDF5ED9D5D2|    Evelyn|   Krueger|     Kelley and Sons|     East Coreyburgh|Lesotho|   (872)960-8087x638|001-729-045-6337x...|emmacameron@sloan...|       2021-04-24|http://www.valenz...|\n",
      "|  236|c0Ea931f3eBb4A5|     Jerry|     Evans|        Velez-Keller|   Port Tracyborough|Lesotho|    785.352.8872x837|+1-848-210-2482x7907|kingkaitlin@mccal...|       2021-12-18|https://www.farre...|\n",
      "|  342|3fbd61a23A8f77A|      Tara|   Harrell|        Cantrell Ltd|      Lake Codymouth|Lesotho|    141-144-3312x223|          9561171411|     ohuber@frye.net|       2021-05-14|http://www.pierce...|\n",
      "|  512|669EBad72e3510F|  Brittany|   Brennan|            Yu Group|       Barberchester|Lesotho|001-319-191-5050x495|       (059)313-1921|underwoodreginald...|       2020-11-18|http://barry-hurs...|\n",
      "|  665|861bbe481B1c14E|    Elijah|  Matthews|   Fletcher and Sons|West Gwendolynbor...|Lesotho|    844-418-5440x531|+1-814-705-5464x8...|  danthony@kline.com|       2020-11-04|http://www.olson....|\n",
      "|  701|B2d25C904230f0C|      Ross|     Booth|Carter, Shepherd ...|           Damonview|Lesotho|        418.961.3832| (775)301-2787x44575|    chodge@casey.com|       2020-03-02|  https://short.org/|\n",
      "| 1228|9e3b2e960Bb92eB|      Seth|    Knight|     Cabrera-Hammond|          Howardberg|Lesotho|   (742)774-5191x270| (728)120-9454x61139|shannontaylor@com...|       2020-11-22|https://watts-boo...|\n",
      "| 1272|eDC0e4A79Fc54bD|     Kayla|  Schaefer|Wang, Livingston ...|          Port Stacy|Lesotho|       (513)657-6247| (071)921-4592x30508|dunnariana@huff-c...|       2020-07-09|   https://ball.biz/|\n",
      "| 1434|9970aF4EeEb07F7|    Regina|     Keith|Keller, Meyers an...|    Port Geneborough|Lesotho|001-567-649-6544x...|          8170418736|    erin95@banks.org|       2020-11-25|https://ramos-all...|\n",
      "| 1759|A9d20092bbfE50f|     Danny|  Martinez|           Tyler LLC|        Caitlinshire|Lesotho|001-360-394-2348x...|+1-176-304-3961x4...|blairfrank@rowlan...|       2021-05-25|http://www.prince...|\n",
      "| 1776|C5fF46CfbC4315e|     Edgar|   Donovan|         Parks-Sloan|         Jodiborough|Lesotho|    734-143-9952x951|        396.049.2274|hunterana@michael...|       2020-10-26|http://www.maynar...|\n",
      "| 2111|B1374e4868a197C|     Allen|Valenzuela|        Gallegos LLC|        South Sandra|Lesotho|          4695348941|+1-883-771-6758x4330|stantonallison@ya...|       2020-07-31|http://www.garner...|\n",
      "| 2129|9dAFfFE3f229524|      Alex|   Everett|Baker, Ellison an...|          Port Diana|Lesotho| (133)204-4607x06078|+1-082-134-9217x0597|clarkmarisa@sims.com|       2021-03-21|https://cervantes...|\n",
      "| 2307|ee8AeB4882035CC|       Amy|   Goodman|Gentry, Lester an...|          Rubenmouth|Lesotho| +1-452-497-2577x723|       (516)921-1978|isabelmccormick@w...|       2021-10-20|https://www.hampt...|\n",
      "| 2378|ACACB8F42c2C791|     Kevin|    Maddox|     Savage and Sons|    South Marvinland|Lesotho|001-674-645-3315x...|    205-902-2191x416|kathrynanderson@r...|       2020-03-19|https://hanson.info/|\n",
      "| 2625|7Cc036BDA9FDecb|   Jasmine|      Frey|Walsh, Barrett an...|        Darrellmouth|Lesotho|          8605024060|   (122)834-4012x991|vmedina@riddle-ki...|       2021-02-27|  https://chung.com/|\n",
      "| 2627|e4c4d7cec29eAff|      Jodi|      Clay|     Hebert and Sons|        Jacksonmouth|Lesotho|       (231)518-6940|  (296)670-6351x8689|keith70@hughes-hu...|       2020-01-09|https://stricklan...|\n",
      "| 4335|BFcE1f201aeBD35|      Kirk| Schneider|       Hickman Group|         West Kelsey|Lesotho|001-913-846-1788x096|        564-552-1338|jorge22@chung-mar...|       2022-04-23|http://www.buckle...|\n",
      "| 4950|EDBFeDBd6375944|    Vernon|    Gamble|          West Group|       Port Kathleen|Lesotho|    001-199-305-9359|   971.844.4679x1176|   qterrell@rowe.com|       2020-12-05|http://www.odonne...|\n",
      "|  210|7Fd8465F2E8F6Fb| Dominique|   Lindsey|          Mcneil PLC|   West Wendychester|Algeria|001-003-500-7259x...|        851-108-8869|  yayers@vasquez.com|       2022-03-28|   http://berry.com/|\n",
      "+-----+---------------+----------+----------+--------------------+--------------------+-------+--------------------+--------------------+--------------------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM local.schema.customers_partitioned_by_country_city TIMESTAMP AS OF '2025-04-21 10:29:54.179';\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2025-04-21 10:29:...| 396991467446470913|               NULL|               true|\n",
      "|2025-04-21 10:38:...|7386602405767638480| 396991467446470913|               true|\n",
      "|2025-04-21 10:44:...|1590301241980826574|7386602405767638480|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM local.schema.customers_partitioned_by_country_city.history;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iceberg schema evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------------+-------+\n",
      "|col_name         |data_type               |comment|\n",
      "+-----------------+------------------------+-------+\n",
      "|Index            |int                     |NULL   |\n",
      "|Customer_Id      |string                  |NULL   |\n",
      "|First_Name       |string                  |NULL   |\n",
      "|Last_Name        |string                  |NULL   |\n",
      "|Company          |string                  |NULL   |\n",
      "|City             |string                  |NULL   |\n",
      "|Country          |string                  |NULL   |\n",
      "|Phone_1          |string                  |NULL   |\n",
      "|Phone_2          |string                  |NULL   |\n",
      "|Email            |string                  |NULL   |\n",
      "|Subscription_Date|date                    |NULL   |\n",
      "|Website          |string                  |NULL   |\n",
      "|                 |                        |       |\n",
      "|# Partitioning   |                        |       |\n",
      "|Part 0           |Country                 |       |\n",
      "|Part 1           |City                    |       |\n",
      "|Part 2           |years(Subscription_Date)|       |\n",
      "+-----------------+------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE TABLE  local.schema.customers_partitioned_by_country_city\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns in iceberg\n",
    "spark.sql(\"ALTER TABLE local.schema.customers_partitioned_by_country_city ALTER COLUMN City AFTER Customer_Id;\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE local.schema.customers_partitioned_by_country_city\n",
    "    ADD COLUMN referrer STRING AFTER Website\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove column\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE local.schema.customers_partitioned_by_country_city \n",
    "    DROP COLUMN referrer\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename column\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE local.schema.customers_partitioned_by_country_city \n",
    "    RENAME COLUMN Website TO Website_name\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change data type\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE local.schema.customers_partitioned_by_country_city\n",
    "    ALTER COLUMN subscription_date SET DATA TYPE TIMESTAMP\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
